---
title: "Text Classification for Interpretation"
description: "Some regularized logistic models for predicting sentiments of movie reviews using interpretive words"
author: "Hope Hunter, Jonas Reger"
date: "11/28/2022"
categories: [Machine Learning, NLP, R, Statistics]
draft: false
freeze: true
page-layout: full
format:
  html:
    code-fold: show
    code-summary: "Reveal the code"
    code-overflow: wrap
    code-tools:
      source: https://github.com/wjonasreger/text_classification_interpretation
    code-block-bg: true
    code-block-border-left: "#56799c"
    code-copy: hover
---

In this notebook, a vocabulary set of highly interpretive terms is built, which a classification model uses to predict the sentiment of a movie review. The data set consists of [50,000 IMDb movie reviews](https://github.com/wjonasreger/data/blob/main/imdb_reviews.tsv), where each review is labelled as positive or negative. A subset of this data set was used in a Kaggle competition: [Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial). Specifically, a classifier based on Regularized Logistic Regression (i.e., Binomial Ridge Regression) is implemented. The source code this notebook and original documentations are viewable [here](https://github.com/wjonasreger/text_classification_interpretation) on GitHub.

## Load Packages & Data

First load the dataset through the URL, or download to access it locally. The following packages will be needed for text preprocessing and analysis.

```{r load packages, warning=FALSE, message=FALSE}
#| code-fold: true
library(text2vec)
library(glmnet)
library(slam)
library(pROC)

set.seed(22)
```

The IMDb movie review dataset has 50,000 rows (i.e., reviews) and 4 columns:

-   **Identification number**: `id`
-   **Sentiment score**: `sentiment` (i.e., `0`=negative, `1`=positive)
-   **10-point score assigned by the reviewer**: `score` (i.e., `1`-`4`=negative sentiment, `7`-`10`=positive sentiment, `5`-`6`=ignored)
-   **User review**: `review`

```{r load data}
#| code-fold: true
# load data
data_url = "https://raw.githubusercontent.com/wjonasreger/data/main/imdb_reviews.tsv"
data = read.table(data_url, stringsAsFactors = FALSE, header = TRUE)
data_head = head(data)
data_head$review = sapply(data_head$review, function(x) paste0(substr(x, 1, 100), '...'))
data_head
```

## Preprocess Data

The reviews are preprocessed by removing HTML tags and other special characters, and tokenization.

```{r clean and tokenize reviews}
# remove HTML tags
data$review = gsub('<.*?>', ' ', data$review)

# tokenize words in reviews
itokens = itoken(data$review, 
                 preprocessor = tolower, 
                 tokenizer = word_tokenizer)
```

A vocabulary set of n-grams is built using the tokenized data, then pruned to remove tokens that are too frequent or infrequent in the data. This removes potential noise from tokens that cannot be learned from very well.

```{r build initial vocabulary}
# words ignored by the vocabulary
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", 
               "you", "your", "yours", "their", "they", "his", "her", 
               "she", "he", "a", "an", "and", "is", "was", "are", "were", 
               "him", "himself", "has", "have", "it", "its", "the", "us")

vocab = create_vocabulary(itokens, 
                          stopwords = stop_words, 
                          ngram = c(1L, 4L)) # uses 1-4 n-grams 

vocab = prune_vocabulary(vocab, 
                         term_count_min = 10, # ignores terms under 10 count
                         doc_proportion_max = 0.5, # ignores terms over 0.5 proportion
                         doc_proportion_min = 0.001) # ignores terms under 0.001 proportion
```

Here is a sample of the vocabulary `vocab`.

```{r vocab sample}
#| code-fold: true
vocab[sample(1:nrow(vocab), 5), ]
```

The initial vocabulary set is then converted into a Document-Term Matrix (DTM) and normalized using TermFrequency-InverseDocumentFrequency (TF-IDF). This structure is convenient for logistic regression.

```{r cat_string function}
#| code-fold: true
# a custom string function towards printing a list of words for webpage
cat_string = function(string_list_in, b=80) {
  string = paste(string_list_in, collapse=", ")
  string_list = strsplit(string, ' ')[[1]]
  
  final_string = '\n\t'
  tmp_line = ''
  
  for (s in 1:length(string_list)) {
    if (s == 1) {
      tmp_string = string_list[s]
    } else {
      tmp_string = paste(tmp_line, string_list[s])
    }
    
    if (nchar(tmp_string) <= b) {
      tmp_line = tmp_string
    } else  if (nchar(tmp_string) > b) {
      final_string = paste0(final_string, tmp_line, "\n\t")
      tmp_line = string_list[s]
    }
    
    if (s == length(string_list)) {
      final_string = paste0(final_string, tmp_line, "\n\t")
    }
  }
  return(final_string)
}
```

```{r create dtm}
dtm = create_dtm(itokens, vocab_vectorizer(vocab))
tfidf = TfIdf$new()
dtm = fit_transform(dtm, tfidf)
```

Here is the size of the vocabulary in `dtm` and some example terms.

```{r vocab examples, results='hold'}
#| code-fold: true
cat(paste0("vocab size: ", length(colnames(dtm))))
cat(paste("\nexample words: ", cat_string(colnames(dtm)[1:25])))
```

## Building a Vocabulary for Interpretation

The goal is to select a smaller vocabulary set under a target size $N_t$ that contains highly interpretive terms. Such terms would intuitively signal positive or negative sentiment to a human reader. For instance, the terms "bad", "worst", "terrible" are clearly negative. Alternatively, the words "acting", "core", "same" are not clearly positive or negative. Of course, the terms would also need to be useful for predictions, so the process of selecting this vocabulary set is carried out in multiple steps. Note that the selected vocabulary set is extracted from the entire data to reduce computational expense. This may not be considered the most appropriate approach but it is as an extension of the tokenization, pruning, and normalization preprocesses. The vocabulary selection is essentially a dimensional reduction method.

The DTM matrix is fitted by a LASSO Logistic Regression model to select terms that are informative towards predicting sentiment of reviews. The motivation is to continue shrinking the vocabulary to keep the most informative terms for eventual predictions. A simple screening method is used to shrink the $20,000+$ vocabulary to some target size $N_t$. The $10,000$ most informative terms are selected from the LASSO model shown below using its degrees of freedom for different $\lambda$ values.

```{r data_classifier_1}
data_classifier1 = glmnet(x = dtm, 
                         y = data$sentiment, 
                         alpha = 1,
                         family='binomial')
```

Here is a helper function to ensure DTM matrices are compatible with a given vocabulary set. This is useful for updating DTM matrices as the vocabulary shrinks, but also for making train-test matrices compatible with each other during model training and evaluation steps.

```{r columnConform function}
columnConform = function(data_matrix, column_names) {
  coverage = colnames(data_matrix) %in% column_names
  miss = !(column_names %in% colnames(data_matrix))
  
  new_data = data_matrix[, coverage]
  empty_data = matrix(0, nrow(new_data), sum(miss))
  colnames(empty_data) = column_names[miss]
  new_data = as.matrix(cbind(new_data, empty_data))
  new_data = new_data[, column_names]

  return(new_data)
}
```

Here the vocabulary is reduced by the LASSO model `data_classifier1` to size $N \leq 10,000$.

```{r shrink to 10000 terms}
# target size is 10000
# select vocabulary set within target size
below_target = data_classifier1$df[data_classifier1$df <= 10000]
df_max_vocab = data_classifier1$df[which.max(below_target)]
df_index = which(data_classifier1$df == df_max_vocab)[1]

# 10,000 terms vocabulary
vocab_lasso = colnames(dtm)[which(data_classifier1$beta[, df_index] != 0)]

# update dtm matrix
dtm = columnConform(dtm, vocab_lasso)

vocab_size = dim(dtm)[2]
```

Here is the size of the vocabulary `vocab_lasso` and some example terms.

```{r vocab_lasso examples, results='hold'}
#| code-fold: true
cat(paste0("vocab size: ", vocab_size))
cat(paste("\nexample words: ", cat_string(vocab_lasso[1:25])))
```

So far the vocabulary is more than sufficient for predictive purposes, but not quite for interpretation. So, less interpretive terms are filtered out using a marginal two-sample t-test. The `slam` package is used to process the large and sparse DTM matrix efficiently. All terms are ordered by the magnitude of their respective t-statistics. The top $2,000$ terms are selected from the ordered vocabulary as they are the most interpretive terms. A few terms appeared exclusively in reviews that were either positive or negative, but were filtered out by the t-test. So, they are added back in since they may be useful for interpretation.

Here the t-test statistics are computed for each term in the DTM matrix.

```{r two-sample t-test}
summ = matrix(0, nrow=vocab_size, ncol=4)

# mean and variances of terms associated with positive reviews
summ[,1] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix( dtm[ data$sentiment==1, ] ), mean)
summ[,2] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix( dtm[ data$sentiment==1, ] ), var)

# mean and variances of terms associated with negative reviews
summ[,3] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix( dtm[ data$sentiment==0, ] ), mean)
summ[,4] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix( dtm[ data$sentiment==0, ] ), var)

# number of positive and negative reviews
n1 = sum(data$sentiment)
n = length(data$sentiment)
n0 = n - n1

# t-statistics for each term
tstats = (summ[,1] - summ[,3]) / sqrt(summ[,2]/n1 + summ[,4]/n0)
```

Here the $10,000$ term vocabulary is filtered by the two-sample t-test and reduced to size $N \leq 2,000$.

```{r selecting 2000 vocabs}
# target size is 2000
# select most interpretive words
words = colnames(dtm)
id = order(abs(tstats), decreasing=TRUE)[1:min(2000, dim(dtm)[2])]

# positive and negative words
pos_vocab = words[ id[ tstats[id] > 0 ] ]
neg_vocab = words[ id[ tstats[id] < 0 ] ]

# checks for terms that appear in only one sentiment
id1 = which(summ[, 2] == 0)
id0 = which(summ[, 4] == 0)

# new vocabulary with ~2000 words
vocab_ttest = words[union(union(id, id1), id0)]

# update the dtm matrix to the new vocabulary
dtm = columnConform(dtm, vocab_ttest)
```

Here is the size of the vocabulary `vocab_ttest` and some example terms. Sets for positive and negative sentiments from the vocabulary are shown as well.

```{r vocab_ttest examples, results='hold'}
#| code-fold: true
cat(paste0("vocab size: ", length(vocab_ttest)))
cat(paste("\nexample words: ", cat_string(vocab_ttest[1:25])))

cat(paste0("\npositive vocab size: ", length(pos_vocab)))
cat(paste("\nexample words: ", cat_string(pos_vocab[1:50])))

cat(paste0("\nnegative vocab size: ", length(neg_vocab)))
cat(paste("\nexample words: ", cat_string(neg_vocab[1:50])))
```

After selecting the best $\sim2000$ terms from the LASSO model and the two-sample t-test, another LASSO model is refitted on the new DTM matrix to further reduce the size of the vocabulary within the final target size $N_t$. Note that this step is identical to the previous LASSO model for terms selection, but with a smaller target size for vocabulary.

```{r data_classifier2}
data_classifier2 = glmnet(x = dtm, 
                          y = data$sentiment, 
                          alpha = 1,
                          family='binomial')
```

Here the vocabulary is reduced by the LASSO model `data_classifier2` to size $N \leq 1,000$, the final target size.

```{r selecting 1000 vocabs}
# target size is 1000
below_target = data_classifier2$df[data_classifier2$df <= 1000]
df_max_vocab = data_classifier2$df[which.max(below_target)]
df_index = which(data_classifier2$df == df_max_vocab)[1]

final_vocab = colnames(dtm)[which(data_classifier2$beta[, df_index] != 0)]
```

Here is the size of the vocabulary `final_vocab` and some example terms.

```{r final_vocab examples, results='hold'}
#| code-fold: true
cat(paste0("final vocab size: ", length(final_vocab)))
cat(paste("\nexample words: ", cat_string(final_vocab[1:100])))
```

## Model Training

First, the train and test data are split from the full data. The datasets undergo the same preprocessing procedure as shown previously, up to TF-IDF transformation. The helper function `preprocessDTM` is defined in the hidden code block below, and `columnConform` is defined in the previous section.

```{r preprocessDTM function}
#| code-fold: true
# preprocess function to create DTM matrices as shown earlier in the notebook
preprocessDTM = function(data, text_column) {
  # remove HTML tags
  data[[text_column]] = gsub('<.*?>', ' ', data[[text_column]])
  
  # tokenize words in reviews
  itokens = itoken(data[[text_column]], 
                   preprocessor = tolower, 
                   tokenizer = word_tokenizer)
  
  # words ignored by the vocabulary
  stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", 
                 "you", "your", "yours", "their", "they", "his", "her", 
                 "she", "he", "a", "an", "and", "is", "was", "are", "were", 
                 "him", "himself", "has", "have", "it", "its", "the", "us")
  
  vocab = create_vocabulary(itokens, 
                            stopwords = stop_words, 
                            ngram = c(1L, 4L)) # uses 1-4 n-grams 
  
  vocab = prune_vocabulary(vocab, 
                           term_count_min = 10, # ignores terms under 10 count
                           doc_proportion_max = 0.5, # ignores terms over 0.5 proportion
                           doc_proportion_min = 0.001) # ignores terms under 0.001 proportion
  
  dtm = create_dtm(itokens, vocab_vectorizer(vocab))
  tfidf = TfIdf$new()
  dtm = fit_transform(dtm, tfidf)
  
  return(dtm)
}
```

```{r test-train split}
# test ids for test-train split
test_id = sample(1:nrow(data), round(0.2 * nrow(data)))

# train and test data split from full data
train = data[-test_id, ]
test = data[test_id, ]

# preprocess train data
dtm_train = preprocessDTM(train, "review")
dtm_train = columnConform(dtm_train, final_vocab)

# preprocess test data
dtm_test = preprocessDTM(test, "review")
dtm_test = columnConform(dtm_test, final_vocab)
```

The train DTM matrix is fitted with a Cross-Validated Ridge Logistic Regression model. The following hyperparameters are used (strict convergence hyperparameters are unnecessary for sufficient model performance, likely due to vocabulary selection procedure):

-   **Number of cross-validation folds**: `5`
-   **Convergence threshold**: `1e-5`
-   **Maximum iterations**: `1e3`

```{r model training}
# training the ridge logistic regression model
train_classifier = cv.glmnet(x = dtm_train, y = train$sentiment, 
                             family='binomial', alpha = 0,
                             nfolds = 5, thresh = 1e-5, maxit = 1e3)
```

## Model Evaluation

Now the Ridge model is evaluated. It returns predicted probabilities that a review has a positive sentiment (i.e., $1 - \hat{P}(positive)$ is the predicted probability that a review is negative).

```{r model predictions}
# test predictions for sentiment of reviews
pred = predict(train_classifier, 
               s = train_classifier$lambda.min, 
               newx = as.matrix(dtm_test),
               type='response')

pred = cbind(test$id, pred)
colnames(pred) = c("id", "prob")

head(pred)
```

```{r auc peek, message=FALSE, echo=FALSE}
# merge predicted probabilities with actual sentiment scores
pred_2 = merge(pred, test, by="id")

# roc object for evaluation
roc_obj = roc(pred_2$sentiment, pred_2$prob)
auc_result = auc(roc_obj)
```

The Ridge model has an Area Under the Receiver Operating Characteristic curve (AUROC) score of `r round(auc_result, 4)`. So, the model can correctly classify at least `r paste0(as.character(100 * round(auc_result, 4)), '%')` of the reviews in the test data. Despite the sufficient performance of the model, the general procedure for vocabulary selection could be further improved towards interpretation. Possible next steps include removing redundant vocabulary terms, adding topic models to enrich representation of terms with ambiguous meanings, or adding neural networks to generalize contexts better. In any case, this procedure demonstrates that interpretive terms can be extracted for highly accurate text classification tasks.

```{r model evaluations, message=FALSE}
# merge predicted probabilities with actual sentiment scores
pred = merge(pred, test, by="id")

# roc object for evaluation
roc_obj = roc(pred$sentiment, pred$prob)
auc_result = auc(roc_obj)

# roc curve and auc score
plot(roc_obj, print.auc=TRUE, print.thres=TRUE, 
     max.auc.polygon=TRUE, auc.polygon=TRUE, auc.polygon.col = "lightblue")
plot(smooth(roc_obj), add=TRUE, col="red")
legend("bottomright", legend=c("Empirical", "Smoothed"),
       col=c("black", "red"), lwd=2)
```
