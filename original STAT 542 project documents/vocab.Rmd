---
title: "Project 3: Movie Review Sentiment Analysis - Building Vocabulary Set"
author: "W. Jonas Reger (wreger2), Hope Hunter (hhunter3)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    df_print: paged
    theme: cosmo
    code_folding: show
    toc: true
    toc_float:
      collapsed: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

<style>
div.blue {background-color:#e6f0ff; border-radius: 0px; padding: 20px;}
code {color: red;}
</style>

<div class="blue">
_**Member Contributions:** Both members worked on assignment individually and collaborated at the end to discuss any issues or discrepancies, and then merge results into one document._
</div>

```{r seed, class.source = 'fold-show'}
set.seed(6594)
start = Sys.time()
```

## Movie Review Data

Here in this notebook, a vocabulary set is built towards predicting the sentiment of a movie review. The data set consists of 50,000 IMDb movie reviews, where each review is labelled as positive or negative.

### Dataset
The data set, [[alldata.tsv](https://liangfgithub.github.io//Data/alldata.tsv)], has 50,000 rows (i.e., reviews) and 4 columns:

* Col 1: "id", the identification number; 
* Col 2: "sentiment", 0 = negative and 1 = positive; 
* Col 3: "score", the 10-point score assigned by the reviewer. Scores 1-4 correspond to negative sentiment; Scores 7-10 correspond to positive sentiment. This data set contains no reviews with score 5 or 6. 
* Col 4: "review". 

A subset of this data set was used in a Kaggle competition: [Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial).



## Building a Vocabulary Set for Interpretation

The general approach in this analysis is to select most informative terms from a document-term matrix that are also most interpretable for eventual predictive modeling processes. This vocabulary set is extracted from the entire data, which may not be the most appropriate method. However, one benefit of selecting a vocabulary set from the entire data is the reduced processing time in model testing (approximately 25%). The motivation of this procedure is to analytically select all terms that may be useful for the predictive modeling procedure, so extracting terms from the entire data is justifiable to this end. Later on in the predictive modeling procedure, the train-test data are simply document-term matrices built on the train-test review data separately and then subsetted according to the globally selected vocabulary set. The emphasis of vocabulary selection procedure is on feature selection or dimensional reduction, while the emphasis for predictive modeling procedure is on feature coefficient shrinkage.

## Load Packages

```{r libraries, message=FALSE, warning=FALSE}
library(text2vec)
library(glmnet)
library(slam)
```

## Building the Vocabulary Set

The reviews undergo a simple pre-processing method to remove html tags and tokenization. A vocabulary is created using the tokens, stop words, and ngram sizes. This vocabulary is then pruned to remove any terms that contribute noise to the data by either being too frequent or too rare. The intuition for this is terms that appear too frequently or infrequently may not be able to contribute much meaning and therefore would be a source of noise in the data. A document-term matrix is constructed using the pruned vocabulary. Finally, tf-idf transformation is applied on the document-term matrix to normalize the data. A helper function is defined below to handle the pre-processing from tokenization to the final document-term matrix.

```{r buildDTM fn}
buildDTM = function(data, text, stop_words, ngrams, min_count, min_dprop, max_dprop) {
  itokens = itoken(text, 
                   preprocessor = tolower, 
                   tokenizer = word_tokenizer)
  vocab = create_vocabulary(itokens, 
                            stopwords = stop_words, 
                            ngram = ngrams)
  vocab = prune_vocabulary(vocab, 
                           term_count_min = min_count, 
                           doc_proportion_max = max_dprop, 
                           doc_proportion_min = min_dprop)
  dtm = create_dtm(itokens, vocab_vectorizer(vocab))
  tfidf = TfIdf$new()
  dtm = fit_transform(dtm, tfidf)
  return(dtm)
}
```

```{r preprocess data}
t1 = Sys.time()
data = read.table("alldata.tsv", stringsAsFactors = FALSE, header = TRUE)
data$review = gsub('<.*?>', ' ', data$review)
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", 
               "you", "your", "yours", "their", "they", "his", "her", 
               "she", "he", "a", "an", "and", "is", "was", "are", "were", 
               "him", "himself", "has", "have", "it", "its", "the", "us")
dtm_data = buildDTM(data=data, text=data$review,
                    stop_words=stop_words, ngrams=c(1L,4L), 
                    min_count=10, min_dprop=0.001, max_dprop=0.5)
print( difftime( Sys.time(), t1, units = 'sec'))
```

Since the vocabulary selection procedure is essentially dimensional reduction, there are a few steps to remove unnecessary terms. The first was pruning as seen previously. Now, a LASSO model (logistic regression) is fitted on the normalized document-term matrix of the full data. The motivation of fitting a LASSO model is simply for term selection rather than predictions.

```{r data_classifier fit}
t1 = Sys.time()
data_classifier = glmnet(x = dtm_data, 
                         y = data$sentiment, 
                         alpha = 1,
                         family='binomial')
print( difftime( Sys.time(), t1, units = 'sec'))
```

The goal of this analysis is to select important terms that are easier to interpret. Therefore, a simple screening method is used. After LASSO is fitted, the document-term matrix is reduced using the vocabulary selected with a threshold ($N_0 = 10000$ in this case) using the model's degrees of freedom values for different lambda values. This is done to remove any terms that are especially unnecessary.

So far the vocabulary that has been selected are useful for predictions, but not quite adequate for interpretability. So, less interpretable terms are filtered out using a marginal two-sample t-test. The previous LASSO procedure is implemented to reduce the processing time in the two-sample t-test as well as filter out noisy terms. The `slam` package is used within the `vocabTTest` function in order to process the large and sparse document-term matrix efficiently. Next, all terms are ordered by the magnitude of their t-statistics (i.e., greater t-statistics magnitudes imply greater interpretability). A smaller vocabulary set is selected from the top $N_1$ terms in the ordered vocabulary ($N_1 = 2000$ in this case). There were also other terms that were ignored by this filtering procedure, which appeared exclusively in reviews with either positive or negative sentiments. So, these terms were added back into the vocabulary set since they may still be useful.

For purposes of ensuring the document-term matrices are compatible across model fitting and predicting procedures under a given vocabulary set, a helper function is created below to conform the matrices to the vocabulary set. This function is useful here for updating the document-term matrix with newly reduced vocabulary sets.

```{r vocabTTest fn}
columnConform = function(data_matrix, column_names) {
  coverage = colnames(data_matrix) %in% column_names
  miss = !(column_names %in% colnames(data_matrix))
  
  new_data = data_matrix[, coverage]
  empty_data = matrix(0, nrow(new_data), sum(miss))
  colnames(empty_data) = column_names[miss]
  new_data = as.matrix(cbind(new_data, empty_data))
  new_data = new_data[, column_names]

  return(new_data)
}

vocabTTest = function(data_y, dtm_matrix, model, model_threshold, vocab_threshold) {
  df_index = which(model$df == model$df[which.max(model$df[model$df <= model_threshold])])[1]
  vocab = colnames(dtm_matrix)[which(model$beta[, df_index] != 0)]
  
  dtm_matrix = columnConform(dtm_matrix, vocab)
  vocab_size = dim(dtm_matrix)[2]
  
  summ = matrix(0, nrow=vocab_size, ncol=4)
  summ[,1] = colapply_simple_triplet_matrix(
    as.simple_triplet_matrix(dtm_matrix[data_y==1, ]), mean)
  summ[,2] = colapply_simple_triplet_matrix(
    as.simple_triplet_matrix(dtm_matrix[data_y==1, ]), var)
  summ[,3] = colapply_simple_triplet_matrix(
    as.simple_triplet_matrix(dtm_matrix[data_y==0, ]), mean)
  summ[,4] = colapply_simple_triplet_matrix(
    as.simple_triplet_matrix(dtm_matrix[data_y==0, ]), var)
  
  n1 = sum(data_y)
  n = length(data_y)
  n0 = n - n1
  
  tstats = (summ[,1] - summ[,3]) / sqrt(summ[,2]/n1 + summ[,4]/n0)
  
  words = colnames(dtm_matrix)
  id = order(abs(tstats), decreasing=TRUE)[1:vocab_threshold]
  pos_vocab = words[id[tstats[id]>0]]
  neg_vocab = words[id[tstats[id]<0]]
  
  id1 = which(summ[, 2] == 0) # same as: which(summ[id0, 1] != 0)
  id0 = which(summ[, 4] == 0) # same as: which(summ[id1, 3] != 0)
  
  select_vocab = words[union(union(id, id1), id0)]
  
  vocab_result = list(
    pos_vocab = pos_vocab,
    neg_vocab = neg_vocab, 
    select_vocab = select_vocab
  )
  
  return(vocab_result)
}
```

```{r global vocab set, results='hold'}
t1 = Sys.time()
# N_0=10000
# N_1=2000
vocab_results = vocabTTest(data_y=data$sentiment, dtm_matrix=dtm_data, 
                model=data_classifier, model_threshold=10000, vocab_threshold=2000)

vocab_set = vocab_results$select_vocab
vocab_set_pos = vocab_results$pos_vocab
vocab_set_neg = vocab_results$neg_vocab
print( difftime( Sys.time(), t1, units = 'sec'))

print(paste0("vocab size: ", length(vocab_set)))
print(paste("example words: ", paste(vocab_set[1:25], collapse=", ")))

print(paste0("positive vocab size: ", length(vocab_set_pos)))
print(paste("example words: ", paste(vocab_set_pos[1:25], collapse=", ")))

print(paste0("negative vocab size: ", length(vocab_set_neg)))
print(paste("example words: ", paste(vocab_set_neg[1:25], collapse=", ")))
```

```{r conform dtm_data}
dtm_data = columnConform(dtm_data, vocab_set)
```

After selecting the best terms via LASSO selection and the two-sample t-test screening method, LASSO is re-applied on the reduced document-term matrix to reduce the global vocabulary set within the final target size, $N_T$. This selection process is identical to the one used in the beginning of the `vocabTTest` function prior to the two-sample t-test. This reduced vocabulary set is the final set under the target size, $N_T = 1000$, with highly interpretable terms that will be used in the predictive modeling procedure.

```{r data_classifier2 fit}
t1 = Sys.time()
data_classifier2 = glmnet(x = dtm_data, 
                          y = data$sentiment, 
                          alpha = 1,
                          family='binomial')
print( difftime( Sys.time(), t1, units = 'sec'))
```

```{r target vocab set, results='hold'}
# N_T=1000
below_target = data_classifier2$df[data_classifier2$df <= 1000]
df_max_vocab = data_classifier2$df[which.max(below_target)]
df_index = which(data_classifier2$df == df_max_vocab)[1]

final_vocab = colnames(dtm_data)[which(data_classifier2$beta[, df_index] != 0)]

write.table(final_vocab, file = "myvocab.txt", 
            quote = FALSE, row.names = FALSE, col.names = FALSE, sep = "\n")

print(paste0("vocab size: ", length(final_vocab)))
print(paste("example words: ", paste(final_vocab[1:25], collapse=", ")))
```

```{r total time}
print( difftime( Sys.time(), start, units = 'sec') )
```





